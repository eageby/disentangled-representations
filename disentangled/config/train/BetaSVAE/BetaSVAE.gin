include 'config.gin'
include 'train/training.gin'

import disentangled.model.objectives
import disentangled.model
import gin.tf.external_configurables

# Training
# ==============================================================================
run_training.model=@model.BetaSVAE()

# Objective
# ==============================================================================
BetaSVAE.objective = @Objective()
Objective.objective_fn = @objectives.betavae

# Model
# ==============================================================================
VAE.latents=%LATENTS

# Encoder
# ==============================================================================
VAE.f_phi = %disentangled.model.networks.conv_4

VAE.f_phi_mean=@mean/tf.keras.layers.Dense()
VAE.f_phi_log_var=@log_var/tf.keras.layers.Dense()

Dense.units=%LATENTS
Dense.activation=None

# Decoder
# ==============================================================================
VAE.f_theta = %disentangled.model.networks.conv_4_transpose
VAE.f_theta_mean=@mean/tf.keras.layers.Conv2DTranspose()
VAE.f_theta_log_var=@log_var/tf.keras.layers.Conv2DTranspose()

Conv2DTranspose.kernel_size=(3,3)
Conv2DTranspose.strides=(1,1)

mean/Conv2DTranspose.activation="sigmoid"
log_var/Conv2DTranspose.activation=None

# Optimizer 
# ==============================================================================
VAE.train.optimizer = @tf.keras.optimizers.Adam()

# Distributions
# ==============================================================================
objectives.betavae.prior_dist=@Laplacian()
objectives.betavae.output_dist = @Bernoulli()

Laplacian.location = 0.0
Laplacian.log_scale = 0.0

# Hyperparameter Sweep
# ==============================================================================
HP_SWEEP_VALUES = [1, 2, 4, 6, 8, 10]

# Metric
# ==============================================================================
mutual_information_gap
